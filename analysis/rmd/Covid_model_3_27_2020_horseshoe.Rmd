---
title: "Covid_model_3_27_2020"
author: "Nicholas Nagle"
date: "3/27/2020"
output: html_document
---

tl;dr: Skip to the cool plots at the end.



# Notation:

Data:

$Y_{it}$ number of cases in county $i$ on data $t$.

$P_i$ population of county $i$.

$X_i$: k-vector of covariates for county $i$.

Latent rates:

$\operatorname{Exp}[Y_{it}] = P_i \lambda_{it}$

$\lambda_{it}$: rate (per person) in county $i$ on date $t$


## Basic idea
The count of cases is a poisson rv with some space-time varying rate.

For an exponentially growing disease, we model the rate as log-linear in time.

"Flattening the curve" can be detected by a declining slope in log-linear space.

So we'll use:

$\log(\lambda_{i,t}) = a_{i} + b_{i,t} t + \theta X_i$

$b_{it} = b_{i,t-1} + \epsilon^{[b]}_{i,t}$

We are especially interested in places/times where $\epsilon^{[b]}_{i,t}$ is negative, as that represents "flattening" the curve.


```{r, warning=FALSE, message=FALSE}
library(sf)
library(tidyverse)
library(rstan)
library(tidybayes)
```

# Load Data

```{r data proc}

geodf <- readr::read_rds('../data/washington-acs.RData')
# Create a unique metro_id
geodf <- geodf %>% 
  mutate(metro_fact = fct_explicit_na(as.factor(csa_title),'non-metro'),
         metro_id = as.numeric(metro_fact),
         county_fact = as.factor(geoid),
         county_id = as.numeric(county_fact))

coviddf <- read_csv("../data/washington-covid19.csv")
# Tasks: convert the date to numeric
coviddf <- coviddf %>%
  mutate(geoid = as.character(geoid), 
         date_r = 1 + as.numeric(date) - min(as.numeric(date))) %>%
  left_join(geodf %>% select(geoid,county_fact, county_id), by=c('geoid'))


covariate_df <- geodf %>% select(acs_median_income_e)
response_df <- coviddf %>%
  select(county_id, date_r, new_cases) %>%
  pivot_wider(names_from = date_r, values_from = new_cases, names_prefix = 'day_',
              values_fill = list(new_cases=0))
```


# Negative Binomial with Gaussian Random Walk on slope
```{stan, output.var="mod_nbh_grw.stan"}
data{
  int N; //number of counties
  int T; //number of time steps
  int K; // number of county-level covariates
  int J1; // number of groups in level 1 (metros?)
  int Y[N,T]; //Cases
  vector[N] Pop; // Population
  int<lower=1,upper=J1> group1[N]; // level 1 groups
  matrix[N,K] X; // county-level covariates
  real pr;
  real<lower=0> scale_global;   // scale for the half-t prior for tau
  real<lower=1> nu_global;      // degrees of freedom for the half-t priors for tau
  real<lower=1> nu_local;       // degrees of freedom for the half-t priors for lambdas
  real<lower=0> slab_scale;     // slab scale for the regularized horseshoe;
  real<lower=0> slab_df;        // slab degrees of freedom for the regularized horseshoe
}
transformed data{
  vector[N] log_pop;
  real horse_p;
  matrix[T,2] X_time;
    for(t in 1:T){
      X_time[t,1] = 1;
      X_time[t,2] = t-1;
    }
  log_pop = log(Pop);
}
parameters{
  vector[K] theta; // coefficients on county-level covariates.
  real a;
  real b;
  vector<upper=3>[N] a0;
  vector<upper=3>[J1] a1;
  real<lower=0,upper=3> tau0;
  real<lower=0,upper=3> tau1;
  real<lower=0,upper=.3> taub;
  matrix[N,T-1] b0_raw;
  real<lower=0> a_gamma;
  matrix<lower=0>[N, T-1] horse_lambda;  // local shrinkage parameter
  real<lower=0> horse_tau;               // global shrinkage paramater
  real<lower=0> horse_caux;
}
transformed parameters{
  vector[N] Xtheta;
  matrix[N,T] b0;
  matrix[N,T] log_lambda;
  matrix<lower=0>[N, T-1] horse_lambda_tilde; // 'truncated' local shrinkage param
  real phi;
  real<lower=0> horse_c;
  horse_c = slab_scale * sqrt(horse_caux);
  for(i in 1:N){
    for(t in 1:(T-1)){
      horse_lambda_tilde[i,t] = sqrt(horse_c^2*horse_lambda[i,t]^2 ./ (horse_c^2+horse_tau^2*horse_lambda[i,t]^2));
    }
  }
  Xtheta = X*theta;
  phi = 1/a_gamma;
  for(i in 1:N){
    b0[i,1] = 0.;
    for(t in 2:T){
      b0[i,t] = b0[i,t-1] + horse_tau * horse_lambda_tilde[i,t-1]* b0_raw[i,t-1];
    }
  }
  for(i in 1:N){
    for(t in 1:T){
      log_lambda[i,t] = Xtheta[i]+ a + tau1*a1[group1[i]] + tau0*a0[i] + 
      (b + b0[i,t]) *t ;
    }
  }
}
model{
  a ~ normal(-15,10);
  a0 ~ normal(0.,1.);
  a1 ~ normal(0., 1.);
  b ~ normal(0,1);
  tau0 ~ cauchy(0.,.1);
  tau1 ~ cauchy(0.,1.);
  taub ~ cauchy(0, 1.);
  to_vector(b0_raw) ~ normal(0.,1.);
  to_vector(horse_lambda) ~ student_t(nu_local, 0., 1.);
  //horse_tau ~ student_t(nu_global, 0, scale_global*taub);
  horse_tau ~ student_t(nu_global, 0, scale_global);
  horse_caux ~ inv_gamma(0.5*slab_df, 0.5*slab_df);
  a_gamma ~ gamma(.001,.001);
  for(i in 1:N) {
    sum(b0_raw[i]) ~ normal(0., T*pr);
    for(t in 1:T){
      Y[i,t] ~ neg_binomial_2_log(log_pop[i] + log_lambda[i,t], phi);
    }
  }
}
generated quantities{
 matrix[N,T] log_mu;
 int Y_sim[N,T];
 for(i in 1:N){
   for(t in 1:T){
     real est;
     log_mu[i,t] = log_pop[i]+log_lambda[i,t];
     est = min([log_mu[i,t], 15.]);
     Y_sim[i,t] = neg_binomial_2_log_rng(est, phi);
   }
 }
}
```




```{r}
stan_data <- list(Pop    = geodf %>% arrange(county_id) %>% pull(acs_total_pop_e) ,
                  group1 = geodf %>% arrange(county_id) %>% pull(metro_id),
                  X      = geodf %>%  st_drop_geometry() %>% 
                    arrange(county_id) %>% 
                    select(inc = acs_median_income_e) %>%
                    mutate(inc = scale(inc)),
                  Y      = response_df %>% select(starts_with('day')) %>% as.matrix(),
                  N      = max(geodf$county_id),
                  K      = 1,
                  J1     = max(geodf$metro_id),
                  T      = max(coviddf$date_r),
                  pr=.001,
                  #scale_global = 1000*(40/(39*31-40))*.025/sqrt(39*31), # .025 is what I think that the sd is.
                  scale_global=1e-4,
                  nu_global = 2,
                  nu_local = 1,
                  slab_scale = .01, # based on prior info about scale of largest coef
                  slab_df = 10
                  )

```


```{r fit mod_nb_grw}
stan_fit <- sampling(object =          mod_nbh_grw.stan,
                     data =            stan_data,
                     iter =            4000, 
                     thin =            4,
                     chains =          8,
                     cores =           8,
                     sample_file =     '../tmp/samples_nbh_grw.csv',
                     diagnostic_file = '../tmp/diagnostic_nbh_grw.csv',
                     append_samples =  FALSE,
                     control =         list(adapt_delta = 0.99, max_treedepth=15))
```

## Convergence checks
```{r}
traceplot(stan_fit, pars=c('a','b','phi','tau0','tau1','horse_c','horse_tau','theta'),  inc_warmup=FALSE)
```

It appears that the tau variables sample slowly.

```{r}
summary(stan_fit, pars=c('a','b','phi','tau0','tau1','taub','horse_tau','theta'))$summary
```

```{r}
pairs(stan_fit, pars=c('a','b','phi','tau0','tau1','horse_c','horse_tau','theta'))
```
Interesting correlation between tau0 and taub! When taub is large, tau0 is more likely to be zero. I think that this is desirable.  When the random walk can explain alot (taub large), then the series specific intercept goes to zero (tau0 -> zero). 

tau1 does appear to barely hit the hard constraint (<3).

## Posterior checks
```{r}
sample <- gather_draws(stan_fit, Y_sim[i,t])
sample2 <- gather_draws(stan_fit, log_mu[i,t])
truth <- response_df %>% 
  pivot_longer(cols=starts_with('day_'), names_to='t', names_prefix='day_', values_to='.value') %>%
  mutate(t=as.integer(t)) %>%
  rename(i=county_id ) %>%
  left_join(geodf %>% st_drop_geometry() %>% select(county_id, county_name),
            by=c('i'='county_id'))

sample2 <- sample2 %>%
  group_by(i,t) %>%
  summarize(est = mean(exp(.value)), q05=exp(quantile(.value, .05)), q95 = exp(quantile(.value, .9))) %>%
  ungroup() %>%
  filter(t>10) %>%
  left_join(geodf %>% st_drop_geometry() %>% select(county_id, county_name),
            by=c('i'='county_id'))
sample <- sample %>% 
  group_by(i,t) %>%
  summarize(est = mean(.value), q05=quantile(.value, .05), q95 = quantile(.value, .9)) %>%
  ungroup() %>%
  filter(t>10) %>%
  left_join(geodf %>% st_drop_geometry() %>% select(county_id, county_name),
            by=c('i'='county_id'))
sample %>%
  ggplot(aes(x=t)) + geom_linerange(aes(ymin=q05, ymax=q95)) + 
  geom_point(data=truth %>% filter(t>10), aes(x=t,y=.value ))  +
  geom_line(data=sample2, aes(x=t, y=est), color='red') +
  facet_wrap(~county_name, scales='free_y')  
```
Black dots=data.  Black lines = 95% posterior credible regions. Red line = posterior expected value.

## Plot the rate

```{r}
sampleb <- gather_draws(stan_fit, b0[i,t])
sampleb %>%
  filter(i==31) %>%
  ggplot(aes(y=.value, x=t)) + 
  stat_halfeye()
```
```{r}
plot_df <- sampleb %>% group_by(i, .draw) %>%
  arrange(t) %>%
  mutate(diff1 = .value-lag(.value,1),
         diff2 = .value-lag(.value,2),
         diff3 = .value-lag(.value,3),
         diff4 = .value-lag(.value,4),
         diff5 = .value-lag(.value,5),
         diff6 = .value-lag(.value,6),
         diff7 = .value-lag(.value,7)) %>%
  ungroup() %>%
  filter(t==31) %>%
  select(-.value) %>%
  pivot_longer(cols = starts_with('diff'),
               names_to='lag',
               names_prefix='diff',
               values_to = 'value' ) %>%
  mutate(variable=lag) %>%
  left_join(geodf %>% st_drop_geometry() %>% select(county_id, county_name),
            by=c('i'='county_id')) 

```


Plot the probability that the curve has flattened between the last day relative to the 7 previous days.


```{r}
ggplot(plot_df %>% filter(i<=20), aes(y=value, x=lag)) + 
  stat_dots(quantiles=20) + 
  facet_wrap(~county_name) + 
  geom_hline(aes(yintercept=0), color='grey25')
```

```{r}
ggplot(plot_df %>% filter(i>20), aes(y=value, x=lag)) + 
  stat_dots(quantiles=20) + 
  facet_wrap(~county_name) + 
  geom_hline(aes(yintercept=0), color='grey25')
```
